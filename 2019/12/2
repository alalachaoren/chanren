今天在看上次在看的那一篇论文，然后大概看完了，文章提出了一种正则化的AEs，在其中，
正则化器将促使这些AEs的编码器在潜在特征空间的某一特定区域将正常数据尽可能地压缩在一起，而重构损失将促使这些AEs防止正常点相互重叠。
在文中作者提出了两种方法，分别是收缩自编码以及狄拉克VAE，这两种方法其实还是看的不是很懂，
通过这两种方法，可以更好的将维数高，稀疏的正常数据与异常数据分割开来，以及该方法对于其中的输入数据的变化的敏感性比较低，对实验结果影响较小。
文中做的实验是先使用自动编码器的前半部分，其实也就是相当于一个特征提取的过程，也就是作者改进的地方，
这种方法能更好的将高维、稀疏的网络数据集正常数据与异常数据分隔开，然后再通过与单类分类器相结合从而进行异常检测。
