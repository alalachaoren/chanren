今天找了一篇17年的Information Sciences的文章‘Incremental anomaly detection using two-layer cluster-based structure’
在这篇文章中他采用的减少数据冗余的方法是将数据先过滤掉重复的数据内容，这个方法或许可以用在处理时序数据中，
文章前面先列举了几种比较常用的集中异常检测的算法，论文中的簇的形状是任意的，没有固定形状，第二步是通过GMM来代表这些正常簇。第三步是用GMM来找到相似的簇，
以此新的实例打上正常或者是异常的标签，最后一步则是对于更新模型。
在第一步中，为了更精确确定正常实例的形状，采用，基于光谱或者是基于密度的方法，这里采用了基于密度的方法，首先找出所给的数据中的所有中心点，
通过中心点的密度（预先到定义好半径），也就是中心点周围临近点的数量是要多余一般点的，然后利用递归，但是仅仅依靠一个中心点并不可以代表整个集群，
我们还学要通过数据的特征来表示完整的数据集群，里面有权重的计算接下来就是模型的更新，先将数据放到缓冲区，然后将新生成的簇也用GMM表示，然后通过比较
该GMM与已经存在的GMM进行对比，然后进行加入或者是删除操作。在里面用到了M-L距离（Kullback–Leibler distance）来计算两个分部之间的距离，
因为采用的M-L距离不是对称的，所以在算两个分布距离时候选择将相互的距离做个平均作为两个分部之间的距离
然后下午在上IT伦理课。
