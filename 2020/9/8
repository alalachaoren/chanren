今天我想到了我在用正常数据进行筛选置信度标签的时候是把正常数据集当成一个类，再用密度函数对置信度进行一个筛选，这样的操作其实是不合理的。
然后我现在就想的是在对这些正常数据聚类的时候，针对其他数据到聚类中心的距离来当做置信度衡量标准，用一个贪心算法策略，当这个数据被分到一个类的置信度高数据中后就把他删除，
这样可以避免在其他类的置信度高数据中出现，导致的重复。
再考虑到每个聚类后簇的大小，来决定每个类中应该留下多少置信度高的标签，而不是每个类留下的数目相同。
如果这样可以的话，创新点应该就够了，正在写代码，看一下效果。
